{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alIIEHibGc3M"
   },
   "source": [
    "## Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "6eDUJ4NtGc3P",
    "outputId": "2480098c-135c-4cbf-9552-018494ee8ff5"
   },
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#  Import and read the attrition data\n",
    "attrition_df = pd.read_csv('https://static.bc-edx.com/ai/ail-v-1-0/m19/lms/datasets/attrition.csv')\n",
    "attrition_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g22aQSY4Gc3Q",
    "outputId": "1f5c13c1-b981-4e40-a7ed-dd3fe6f1b81e"
   },
   "outputs": [],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "attrition_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50vMgBEnJbfM"
   },
   "outputs": [],
   "source": [
    "# Create y_df with the Attrition and Department columns\n",
    "y_df = attrition_df[['Attrition', 'Department']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Virka0zLGc3R",
    "outputId": "dd5aee3a-9458-4ba6-e857-1b234de40915"
   },
   "outputs": [],
   "source": [
    "# Create a list of at least 10 column names to use as X data\n",
    "X_columns = ['Age', \n",
    "            'BusinessTravel', \n",
    "            'DistanceFromHome', \n",
    "            'Education', \n",
    "            'EducationField', \n",
    "            'EnvironmentSatisfaction', \n",
    "            'HourlyRate', \n",
    "            'JobInvolvement',\n",
    "            'JobLevel',\n",
    "            'JobRole',\n",
    "            'JobSatisfaction',\n",
    "            'MaritalStatus',\n",
    "            'NumCompaniesWorked',\n",
    "            'OverTime',\n",
    "            'PercentSalaryHike',\n",
    "            'PerformanceRating',\n",
    "            'RelationshipSatisfaction',\n",
    "            'StockOptionLevel',\n",
    "            'TotalWorkingYears',\n",
    "            'TrainingTimesLastYear',\n",
    "            'WorkLifeBalance',\n",
    "            'YearsAtCompany',\n",
    "            'YearsInCurrentRole',\n",
    "            'YearsSinceLastPromotion',\n",
    "            'YearsWithCurrManager']\n",
    "\n",
    "\n",
    "# Create X_df using your selected columns\n",
    "X_df = attrition_df[X_columns]\n",
    "\n",
    "# Show the data types for X_df\n",
    "X_df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaJfdOGUMHMR"
   },
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYubUJqiLCSp",
    "outputId": "53f31721-571c-4c94-d13e-25a715749593"
   },
   "outputs": [],
   "source": [
    "# Convert your X data to numeric data types however you see fit\n",
    "# Add new code cells as necessary\n",
    "\n",
    "cols_to_encode = ['BusinessTravel', 'EducationField', 'JobRole', 'MaritalStatus', 'OverTime']\n",
    "\n",
    "# Determine the number of value counts in each column.\n",
    "for col in cols_to_encode:\n",
    "    print(f'{col}: {X_train[col].value_counts()}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the columns\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "\n",
    "\n",
    "# Fit and encode the training data\n",
    "X_train_encoded = encoder.fit_transform(X_train[cols_to_encode])\n",
    "X_test_encoded = encoder.transform(X_test[cols_to_encode])\n",
    "\n",
    "# Convert the encoded arrays to DataFrames\n",
    "X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(cols_to_encode))\n",
    "X_test_encoded_df = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(cols_to_encode))\n",
    "\n",
    "print(X_train_encoded_df.dtypes)\n",
    "print(X_test_encoded_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns to scale\n",
    "cols_to_scale = ['Age', \n",
    "                'DistanceFromHome',\n",
    "                'Education',\n",
    "                'EnvironmentSatisfaction', \n",
    "                'HourlyRate',\n",
    "                'JobInvolvement',\n",
    "                'JobLevel',\n",
    "                'JobSatisfaction', \n",
    "                'NumCompaniesWorked', \n",
    "                'PercentSalaryHike',\n",
    "                'PerformanceRating',\n",
    "                'RelationshipSatisfaction',\n",
    "                'StockOptionLevel', \n",
    "                'TotalWorkingYears', \n",
    "                'TrainingTimesLastYear',\n",
    "                'WorkLifeBalance', \n",
    "                'YearsAtCompany', \n",
    "                'YearsInCurrentRole', \n",
    "                'YearsSinceLastPromotion', \n",
    "                'YearsWithCurrManager']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWA-aIA5Gc3T"
   },
   "outputs": [],
   "source": [
    "# Create a StandardScaler\n",
    "scale = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler to the training data\n",
    "X_train_scaled = scale.fit_transform(X_train[cols_to_scale])\n",
    "\n",
    "\n",
    "# Scale the training and testing data\n",
    "X_test_scaled = scale.transform(X_test[cols_to_scale])\n",
    "\n",
    "# Convert the scaled arrays to DataFrames\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=cols_to_scale)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=cols_to_scale)\n",
    "\n",
    "X_train_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the scaled and encoded columns\n",
    "X_train_df = pd.concat([X_train_encoded_df, X_train_scaled_df], axis=1)\n",
    "X_test_df = pd.concat([X_test_encoded_df, X_test_scaled_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df['Department'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-z0Mky8vQSz4",
    "outputId": "debefc85-c20b-48f5-f4d9-91eadd65d36a"
   },
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder for the Department column\n",
    "dept_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop=None)\n",
    "\n",
    "# Fit the encoder to the training data\n",
    "dept_encoder.fit(np.array(y_train['Department'].values.reshape(-1, 1)))\n",
    "\n",
    "# Create two new variables by applying the encoder\n",
    "# to the training and testing data\n",
    "y_dept_train = dept_encoder.transform(np.array(y_train['Department'].values.reshape(-1, 1)))\n",
    "y_dept_test = dept_encoder.transform(np.array(y_test['Department'].values.reshape(-1, 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df['Attrition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-G4DSpvFRrk4",
    "outputId": "9842e948-8a55-4b80-8fac-f96714e85589"
   },
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder for the Attrition column\n",
    "atr_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop=None)\n",
    "\n",
    "# Fit the encoder to the training data\n",
    "# atr_encoder.fit(np.array(y_train['Attrition'].values.reshape(-1, 1)))\n",
    "atr_encoder.fit(np.array(y_train['Attrition'].values.reshape(-1, 1)))\n",
    "\n",
    "# Create two new variables by applying the encoder\n",
    "# to the training and testing data\n",
    "y_atr_train = atr_encoder.transform(np.array(y_train['Attrition'].values.reshape(-1, 1)))\n",
    "y_atr_test = atr_encoder.transform(np.array(y_test['Attrition'].values.reshape(-1, 1)))\n",
    "y_atr_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykNmu_WWGc3T"
   },
   "source": [
    "## Create, Compile, and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUptZqmSGc3T"
   },
   "outputs": [],
   "source": [
    "# Find the number of columns in the X training data\n",
    "number_input_features = len(X_train_df.columns)\n",
    "# number_input_features = X_train_df.shape[1]\n",
    "\n",
    "# Create the input layer\n",
    "input_layer = layers.Input(shape=(number_input_features,), name='input')\n",
    "\n",
    "# Create at least two shared layers\n",
    "shared_layer1 = layers.Dense(128, activation='relu', name='shared_layer1')(input_layer)\n",
    "shared_layer2 = layers.Dense(64, activation='relu', name='shared_layer2')(shared_layer1)\n",
    "shared_layer3 = layers.Dense(32, activation='relu', name='shared_layer3')(shared_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JukjTm2yTEqd"
   },
   "outputs": [],
   "source": [
    "# Create a branch for Department\n",
    "# with a hidden layer and an output layer\n",
    "\n",
    "# Create the hidden layer\n",
    "dept_hidden_layer = layers.Dense(16, activation='relu', name='dept_hidden_layer')(shared_layer3)\n",
    "\n",
    "# Create the output layer\n",
    "dept_output_layer = layers.Dense(y_dept_train.shape[1], activation='softmax', name='dept_output_layer')(dept_hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OqhUiOJUBkR"
   },
   "outputs": [],
   "source": [
    "# Create a branch for Attrition\n",
    "# with a hidden layer and an output layer\n",
    "\n",
    "# Create the hidden layer\n",
    "atr_hidden_layer = layers.Dense(16, activation='relu', name='atr_hidden_layer')(shared_layer3)\n",
    "\n",
    "# Create the output layer\n",
    "atr_output_layer = layers.Dense(y_atr_train.shape[1], activation='sigmoid', name='atr_output_layer')(atr_hidden_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twmuejdxGc3T",
    "outputId": "25096308-b68b-42e4-e4ea-ae82e97c435a"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model(\n",
    "    inputs=input_layer, \n",
    "    outputs=[dept_output_layer, atr_output_layer],\n",
    "    name='model')\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "                loss={'dept_output_layer': 'categorical_crossentropy',\n",
    "                    'atr_output_layer': 'categorical_crossentropy'},\n",
    "                metrics={'dept_output_layer': 'accuracy',\n",
    "                    'atr_output_layer': 'accuracy'})\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8oGy0dpGc3U",
    "outputId": "cc667d43-28cf-42d4-d719-c2bc02888d30"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "        X_train_df, \n",
    "        {'dept_output_layer': y_dept_train, \n",
    "        'atr_output_layer': y_atr_train}, \n",
    "        epochs=100, \n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VsMoaQlgGc3U",
    "outputId": "1bd4e601-e964-4abc-ad83-aeecf6b696be"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model with the testing data\n",
    "test_results = model.evaluate(X_test_df, \n",
    "            {'dept_output_layer': y_dept_test, \n",
    "            'atr_output_layer': y_atr_test})\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlCtlHi0Vt54",
    "outputId": "bc21ef3e-80c2-4b38-9c29-79515bc23dec"
   },
   "outputs": [],
   "source": [
    "# Print the accuracy for both department and attrition\n",
    "print(f\"Department Accuracy: {test_results[2]*100}\")\n",
    "print(f\"Attrition Accuracy: {test_results[1]*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGSyfsZfWOQM"
   },
   "source": [
    "# Summary\n",
    "\n",
    "In the provided space below, briefly answer the following questions.\n",
    "\n",
    "1. Is accuracy the best metric to use on this data? Why or why not?\n",
    "\n",
    "2. What activation functions did you choose for your output layers, and why?\n",
    "\n",
    "3. Can you name a few ways that this model might be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi9SLpFnWvbF"
   },
   "source": [
    "YOUR ANSWERS HERE\n",
    "\n",
    "1. No, accuracy is not the best metric to use on this data given the imbalance identified by the value counts for Department and Attrition features. The model accuracy is      measuring the proportion of correctly predicated instances out of total instances. A higher probability for one class may result in a high accuracy score by simply predicting the majority class, even though the model may not be correctly identifying the minority class.\n",
    "\n",
    "2. For the dept_output_layer, the softmax activation function was chosen because it is appropriate for multi-class classification problems. The softmax function converts the raw output scores into probabilities that sum to 1, allowing the model to make a probabilistic prediction for each class, and the class with the highest probability is chosen as the predicted class.\n",
    "For the atr_output_layer, the sigmoid activation function was chosen because it is appropriate for binary classification problems. The sigmoid function outputs a probability value between 0 and 1, which can be interpreted as the likelihood of the positive class. A threshold (usually 0.5) is then used to decide the class label.\n",
    "\n",
    "3. The model score can be improved through the following ways:\n",
    "Data Preprocessing - Adjusting the class weights or using resampling techniques to address the imbalance in the minority class. We could also creating or transforming features to provide a better way to capture the underlying patterns in the data.\n",
    "Model Architecture - Changing the model architecture by adding more layers, increasing the number of neurons in the layers, or using a different activation function.\n",
    "Hyperparameter Tuning - Changing the hyperparameters such as the learning rate, batch size, optimizer, loss function, and number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
